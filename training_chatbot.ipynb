{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jxvB-WsnWFXC",
        "outputId": "20970256-e23e-460e-b241-4488523bc777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting absl-py==0.8.1\n",
            "  Downloading absl-py-0.8.1.tar.gz (103 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▏                            | 10 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 20 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 30 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 40 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 61 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 81 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 92 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 103 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting astor==0.7.1\n",
            "  Downloading astor-0.7.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting certifi==2019.9.11\n",
            "  Downloading certifi-2019.9.11-py2.py3-none-any.whl (154 kB)\n",
            "\u001b[K     |████████████████████████████████| 154 kB 29.9 MB/s \n",
            "\u001b[?25hCollecting Click==7.0\n",
            "  Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting Flask==1.1.1\n",
            "  Downloading Flask-1.1.1-py2.py3-none-any.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting gast==0.3.2\n",
            "  Downloading gast-0.3.2.tar.gz (13 kB)\n",
            "Collecting grpcio==1.23.0\n",
            "  Downloading grpcio-1.23.0-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 42.8 MB/s \n",
            "\u001b[?25hCollecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 29.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: itsdangerous==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (1.1.0)\n",
            "Collecting Jinja2==2.10.3\n",
            "  Downloading Jinja2-2.10.3-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 57.6 MB/s \n",
            "\u001b[?25hCollecting Keras-Applications==1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.2 MB/s \n",
            "\u001b[?25hCollecting Keras-Preprocessing==1.1.0\n",
            "  Downloading Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 697 kB/s \n",
            "\u001b[?25hCollecting Markdown==3.1.1\n",
            "  Downloading Markdown-3.1.1-py2.py3-none-any.whl (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 6.1 MB/s \n",
            "\u001b[?25hCollecting MarkupSafe==1.1.1\n",
            "  Downloading MarkupSafe-1.1.1-cp37-cp37m-manylinux2010_x86_64.whl (33 kB)\n",
            "Collecting mkl-fft\n",
            "  Downloading mkl_fft-1.3.1-16-cp37-cp37m-manylinux2014_x86_64.whl (241 kB)\n",
            "\u001b[K     |████████████████████████████████| 241 kB 59.5 MB/s \n",
            "\u001b[?25hCollecting mkl-random\n",
            "  Downloading mkl_random-1.2.2-16-cp37-cp37m-manylinux2014_x86_64.whl (379 kB)\n",
            "\u001b[K     |████████████████████████████████| 379 kB 57.6 MB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 50.7 MB/s \n",
            "\u001b[?25hCollecting numpy==1.16.5\n",
            "  Downloading numpy-1.16.5-cp37-cp37m-manylinux1_x86_64.whl (17.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3 MB 248 kB/s \n",
            "\u001b[?25hCollecting Pillow==6.2.0\n",
            "  Downloading Pillow-6.2.0-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 41.2 MB/s \n",
            "\u001b[?25hCollecting protobuf==3.10.0\n",
            "  Downloading protobuf-3.10.0-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 52.0 MB/s \n",
            "\u001b[?25hCollecting pyreadline==2.1\n",
            "  Downloading pyreadline-2.1.zip (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 58.0 MB/s \n",
            "\u001b[?25hCollecting scipy==1.3.1\n",
            "  Downloading scipy-1.3.1-cp37-cp37m-manylinux1_x86_64.whl (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting six==1.12.0\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting tensorboard==1.13.1\n",
            "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 43.7 MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.13.2\n",
            "  Downloading tensorflow-1.13.2-cp37-cp37m-manylinux1_x86_64.whl (92.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 92.7 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.13.0\n",
            "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 70.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 27)) (1.1.0)\n",
            "Collecting tflearn==0.3.2\n",
            "  Downloading tflearn-0.3.2.tar.gz (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 7.1 MB/s \n",
            "\u001b[?25hCollecting Werkzeug==0.16.0\n",
            "  Downloading Werkzeug-0.16.0-py2.py3-none-any.whl (327 kB)\n",
            "\u001b[K     |████████████████████████████████| 327 kB 39.7 MB/s \n",
            "\u001b[?25hCollecting wincertstore==0.2\n",
            "  Downloading wincertstore-0.2-py2.py3-none-any.whl (8.8 kB)\n",
            "Collecting gunicorn\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=36 in /usr/local/lib/python3.7/dist-packages (from Markdown==3.1.1->-r requirements.txt (line 13)) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard==1.13.1->-r requirements.txt (line 24)) (0.37.1)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Collecting mkl-fft\n",
            "  Downloading mkl_fft-1.3.0-1-cp37-cp37m-manylinux2014_x86_64.whl (240 kB)\n",
            "\u001b[K     |████████████████████████████████| 240 kB 42.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: mkl in /usr/local/lib/python3.7/dist-packages (from mkl-fft->-r requirements.txt (line 15)) (2019.0)\n",
            "Collecting dpcpp_cpp_rt\n",
            "  Downloading dpcpp_cpp_rt-2022.1.0-py2.py3-none-manylinux1_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 41.4 MB/s \n",
            "\u001b[?25hCollecting mkl-random\n",
            "  Downloading mkl_random-1.2.1-3-cp37-cp37m-manylinux2014_x86_64.whl (378 kB)\n",
            "\u001b[K     |████████████████████████████████| 378 kB 58.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: intel-openmp==2022.1.0 in /usr/local/lib/python3.7/dist-packages (from dpcpp_cpp_rt->mkl-fft->-r requirements.txt (line 15)) (2022.1.0)\n",
            "Collecting intel-cmplr-lic-rt==2022.1.0\n",
            "  Downloading intel_cmplr_lic_rt-2022.1.0-py2.py3-none-manylinux1_x86_64.whl (18 kB)\n",
            "Collecting intel-opencl-rt==2022.1.0\n",
            "  Downloading intel_opencl_rt-2022.1.0-py2.py3-none-manylinux1_x86_64.whl (232.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 232.4 MB 5.5 kB/s \n",
            "\u001b[?25hCollecting intel-cmplr-lib-rt==2022.1.0\n",
            "  Downloading intel_cmplr_lib_rt-2022.1.0-py2.py3-none-manylinux1_x86_64.whl (37.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 37.2 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting tbb==2021.*\n",
            "  Downloading tbb-2021.6.0-py2.py3-none-manylinux1_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 49.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: absl-py, gast, nltk, pyreadline, tflearn\n",
            "  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for absl-py: filename=absl_py-0.8.1-py3-none-any.whl size=121174 sha256=6a7c4d68b9a301d08f01bd26aecbc166323205ca27d162fc4aeb216467c21d2d\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/91/e3/0fced4f5fbc0a051a5667096826186c9ff60f2d0e9bf0f1cdc\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.3.2-py3-none-any.whl size=9694 sha256=80707a5c59603706e06b8b68485b747f9e7a9702380452a15855562c4173b990\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/7b/89/365a82e420c11db9b43d34918045faa8004748a1d97597930d\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449921 sha256=c48cd8155f01e2bd61d86c1960e1a3589645e28797a10d66d6b6463752e415a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "  Building wheel for pyreadline (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyreadline: filename=pyreadline-2.1-py3-none-any.whl size=93834 sha256=d3b09a9f517109727272681a813d24e06c9c1136f74e30b77dc07be9110ed08b\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/6e/d4/7c4b7bc22c090baf4f470e7f35c4f22206277229bdb6607df6\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.3.2-py3-none-any.whl size=128207 sha256=b3a322192ce21905a912d881d3ac3c5b3737a95e84185d46a4dd8c0bf68a1ada\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/b7/55/b387552a28a2fefa559a362ffe17c97fac7e618ef948930b3c\n",
            "Successfully built absl-py gast nltk pyreadline tflearn\n",
            "Installing collected packages: tbb, six, numpy, intel-cmplr-lic-rt, Werkzeug, protobuf, mock, MarkupSafe, Markdown, intel-opencl-rt, intel-cmplr-lib-rt, h5py, grpcio, absl-py, tensorflow-estimator, tensorboard, Pillow, Keras-Preprocessing, Keras-Applications, Jinja2, gast, dpcpp-cpp-rt, Click, astor, wincertstore, tflearn, tensorflow, scipy, pyreadline, nltk, mkl-random, mkl-fft, gunicorn, Flask, certifi\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.0.1\n",
            "    Uninstalling MarkupSafe-2.0.1:\n",
            "      Successfully uninstalled MarkupSafe-2.0.1\n",
            "  Attempting uninstall: Markdown\n",
            "    Found existing installation: Markdown 3.3.7\n",
            "    Uninstalling Markdown-3.3.7:\n",
            "      Successfully uninstalled Markdown-3.3.7\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.46.1\n",
            "    Uninstalling grpcio-1.46.1:\n",
            "      Successfully uninstalled grpcio-1.46.1\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.0.0\n",
            "    Uninstalling absl-py-1.0.0:\n",
            "      Successfully uninstalled absl-py-1.0.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: Keras-Preprocessing\n",
            "    Found existing installation: Keras-Preprocessing 1.1.2\n",
            "    Uninstalling Keras-Preprocessing-1.1.2:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "  Attempting uninstall: Jinja2\n",
            "    Found existing installation: Jinja2 2.11.3\n",
            "    Uninstalling Jinja2-2.11.3:\n",
            "      Successfully uninstalled Jinja2-2.11.3\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: Click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "  Attempting uninstall: astor\n",
            "    Found existing installation: astor 0.8.1\n",
            "    Uninstalling astor-0.8.1:\n",
            "      Successfully uninstalled astor-0.8.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0+zzzcolab20220506162203\n",
            "    Uninstalling tensorflow-2.8.0+zzzcolab20220506162203:\n",
            "      Successfully uninstalled tensorflow-2.8.0+zzzcolab20220506162203\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Attempting uninstall: Flask\n",
            "    Found existing installation: Flask 1.1.4\n",
            "    Uninstalling Flask-1.1.4:\n",
            "      Successfully uninstalled Flask-1.1.4\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2021.10.8\n",
            "    Uninstalling certifi-2021.10.8:\n",
            "      Successfully uninstalled certifi-2021.10.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 0.20.2 requires numpy>=1.18, but you have numpy 1.16.5 which is incompatible.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.16.5 which is incompatible.\n",
            "tensorflow-metadata 1.8.0 requires absl-py<2.0.0,>=0.9, but you have absl-py 0.8.1 which is incompatible.\n",
            "tensorflow-metadata 1.8.0 requires protobuf<4,>=3.13, but you have protobuf 3.10.0 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.16.5 which is incompatible.\n",
            "pywavelets 1.3.0 requires numpy>=1.17.3, but you have numpy 1.16.5 which is incompatible.\n",
            "pyerfa 2.0.0.1 requires numpy>=1.17, but you have numpy 1.16.5 which is incompatible.\n",
            "pyarrow 6.0.1 requires numpy>=1.16.6, but you have numpy 1.16.5 which is incompatible.\n",
            "pandas 1.3.5 requires numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\", but you have numpy 1.16.5 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.16.5 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.13.2 which is incompatible.\n",
            "jaxlib 0.3.7+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.16.5 which is incompatible.\n",
            "jax 0.3.8 requires numpy>=1.19, but you have numpy 1.16.5 which is incompatible.\n",
            "googleapis-common-protos 1.56.1 requires protobuf>=3.15.0, but you have protobuf 3.10.0 which is incompatible.\n",
            "google-colab 1.0.0 requires astor~=0.8.1, but you have astor 0.7.1 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-python-client 1.12.11 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-core 1.31.5 requires protobuf>=3.12.0; python_version > \"3\", but you have protobuf 3.10.0 which is incompatible.\n",
            "google-api-core 1.31.5 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "bokeh 2.3.3 requires pillow>=7.1.0, but you have pillow 6.2.0 which is incompatible.\n",
            "astropy 4.3.1 requires numpy>=1.17, but you have numpy 1.16.5 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Click-7.0 Flask-1.1.1 Jinja2-2.10.3 Keras-Applications-1.0.8 Keras-Preprocessing-1.1.0 Markdown-3.1.1 MarkupSafe-1.1.1 Pillow-6.2.0 Werkzeug-0.16.0 absl-py-0.8.1 astor-0.7.1 certifi-2019.9.11 dpcpp-cpp-rt-2022.1.0 gast-0.3.2 grpcio-1.23.0 gunicorn-20.1.0 h5py-2.10.0 intel-cmplr-lib-rt-2022.1.0 intel-cmplr-lic-rt-2022.1.0 intel-opencl-rt-2022.1.0 mkl-fft-1.3.0 mkl-random-1.2.1 mock-4.0.3 nltk-3.4.5 numpy-1.16.5 protobuf-3.10.0 pyreadline-2.1 scipy-1.3.1 six-1.12.0 tbb-2021.6.0 tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0 tflearn-0.3.2 wincertstore-0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "astor",
                  "google",
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBNozbhkbX0Q",
        "outputId": "8cd0e373-48e0-42d5-ecc4-4205b66414b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import os\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import numpy as np\n",
        "import tflearn\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "#Loading Data\n",
        "with open(\"intents.json\") as file:\n",
        "\tdata = json.load(file)\n",
        "  \n",
        "#Initializing empty lists\n",
        "words = []\n",
        "labels = []\n",
        "docs_x = []\n",
        "docs_y = []\n",
        "\n",
        "#Looping through our data\n",
        "for intent in data['intents']:\n",
        "\tfor pattern in intent['patterns']:\n",
        "\t\tpattern = pattern.lower()\n",
        "    \t\t#Creating a list of words\n",
        "\t\twrds = nltk.word_tokenize(pattern)\n",
        "\t\twords.extend(wrds)\n",
        "\t\tdocs_x.append(wrds)\n",
        "\t\tdocs_y.append(intent['tag'])\n",
        "\n",
        "\tif intent['tag'] not in labels:\n",
        "\t  labels.append(intent['tag'])\n",
        "  \n",
        "stemmer = PorterStemmer()\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in \"?\"]\n",
        "words = sorted(list(set(words)))\n",
        "labels = sorted(labels)\n",
        "\n",
        "training = []\n",
        "output = []\n",
        "\n",
        "out_empty = [0 for _ in range(len(labels))]\n",
        "for x,doc in enumerate(docs_x):\n",
        "\tbag = []\n",
        "\twrds = [stemmer.stem(w) for w in doc]\n",
        "\tfor w in words:\n",
        "\t\tif w in wrds:\n",
        "\t\t\tbag.append(1)\n",
        "\t\telse:\n",
        "\t\t\tbag.append(0)\n",
        "\toutput_row = out_empty[:]\n",
        "\toutput_row[labels.index(docs_y[x])] = 1\n",
        "\ttraining.append(bag)\n",
        "\toutput.append(output_row)\n",
        "#Converting training data into NumPy arrays\n",
        "training = np.array(training)\n",
        "output = np.array(output)\n",
        "\n",
        "#Saving data to disk\n",
        "with open(\"data.pickle\",\"wb\") as f:\n",
        "\tpickle.dump((words, labels, training, output),f)\n",
        " \n",
        "tf.reset_default_graph()\n",
        "\n",
        "net = tflearn.input_data(shape = [None, len(training[0])])\n",
        "net = tflearn.fully_connected(net,8)\n",
        "net = tflearn.fully_connected(net,8)\n",
        "net = tflearn.fully_connected(net,len(output[0]), activation = \"softmax\")\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "model = tflearn.DNN(net)\n",
        "model.fit(training, output, n_epoch = 200, batch_size = 8, show_metric = True)\n",
        "model.save(\"model.tflearn\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLm3rFinWPtB",
        "outputId": "6cfe4f10-b869-42a7-bb1a-6f22d64e7435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step: 9599  | total loss: \u001b[1m\u001b[32m0.14002\u001b[0m\u001b[0m | time: 0.170s\n",
            "| Adam | epoch: 200 | loss: 0.14002 - acc: 0.9355 -- iter: 376/377\n",
            "Training Step: 9600  | total loss: \u001b[1m\u001b[32m0.16216\u001b[0m\u001b[0m | time: 0.178s\n",
            "| Adam | epoch: 200 | loss: 0.16216 - acc: 0.9420 -- iter: 377/377\n",
            "--\n",
            "INFO:tensorflow:/content/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training "
      ],
      "metadata": {
        "id": "n9fCkX5506mF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e5b9df8-6e53-4dbe-a715-f7474c11cc1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 1, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMXSjRgfXbeC",
        "outputId": "aecd0112-c759-4cde-d92f-6438e6c14e62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "n8mVhHhgYlAg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}